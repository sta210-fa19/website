---
title: "Multiple Linear Regression"
subtitle: "Model Assessment"
author: "Dr. Maria Tackett"
date: "09.30.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 4,
	message = FALSE,
	warning = FALSE, 
	dpi = 300
)
```

class: middle, center

### [Click for PDF of slides](10-model-assessment.pdf)

---

### Announcements

- Lab 05 **due Tuesday at 11:59p**

- HW 03 **due Wednesday at 11:59p**
    
- [Reading 06](https://www2.stat.duke.edu/courses/Fall19/sta210.001/reading/reading-06.html) for Wednesday 
    
---

## R packages

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(Sleuth3) # ex0824 data
library(cowplot) # use plot_grid function

```

---

class: middle, center

### Log Transformations 

---

## Respiratory Rate vs. Age

- A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate. 

- The data contain the respiratory rate for 618 children ages 15 days to 3 years.

- **Variables**: 
    - <font class="vocab">`Age`</font>: age in months
    - <font class="vocab">`Rate`</font>: respiratory rate (breaths per minute)
    
```{r echo = F}
respiratory <- ex0824 %>%
  mutate(log_rate = log(Rate))
```

---

### Log transformation on $y$

```{r}
log_model <- lm(log_rate ~ Age, data = respiratory) 
  kable(tidy(log_model, conf.int = TRUE), format = "markdown", digits = 3)
```

.alert[
$$\hat{\text{log rate}} = 3.845 - 0.019 \times \text{Age}$$
]

- <font class = "vocab">Slope: </font> For every one month increase in Age, we expect the median respiratory rate to be multiplied by a factor of $\exp\{-0.019\} = 1.019$ breaths per minute.

- <font class = "vocab">Intercept: </font> The expected respiratory rate for a child who is 0 months old (a newborn) is $\exp\{3.845\} = 46.76$ breaths per minute.

---

### Confidence interval for $\beta_j$

- The confidence interval for the coefficient of $x$ describing its relationship with $\log(y)$ is

$$\hat{\beta}_j \pm t^* SE(\hat{\beta_j})$$

--

- The confidence interval for the coefficient of $x$ describing its relationship with $y$ is

.alert[
$$\exp\big\{\hat{\beta}_j \pm t^* SE(\hat{\beta_j})\big\}$$
]

---

### Coefficient of `Age`

```{r echo = F}
kable(tidy(log_model, conf.int=TRUE),format="html", digits=3)
```

.alert[
The 95% confidence interval for the coefficient of `Age` in terms of `Rate`: 

$$[\exp\{-0.02\}, \exp\{-0.018\}] = [0.981, 0.982]$$ 
]

<font class = "vocab">Interpretation: </font> We are 95% confident that for each additional month in age, we can expect the median respiratory rate to be multiplied by a factor of 0.981 to 0.982.


---

### Log Transformation on $x$

.pull-left[
```{r,echo=F}
set.seed(1)
s <- ggplot2::diamonds %>% sample_n(100)
ggplot(data=s,aes(x=carat,y=log(price)))+
  geom_point(color="blue")+
  ggtitle("Scatterplot")+
  xlab("X")+
  ylab("Y") + 
  theme(plot.title = element_text(hjust = 0.5,size=20))
```
]

.pull-right[
```{r,echo=F}
mod2 <- lm(log(price) ~ carat, data=s)
s <- s %>% mutate(residuals = resid(mod2))
ggplot(data=s,aes(x=carat,y=residuals)) + 
geom_point()+
geom_hline(yintercept=0,color="red") +
  ggtitle("Residual Plot")+
  xlab("X")+
  ylab("residuals") + 
  theme(plot.title = element_text(hjust = 0.5,size=20))
```
]

- Try a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$

---

### Model with Transformation on $x$

.alert[
$$y = \beta_0 + \beta_1 \log(x)$$
]
<br> 

--

- <font class="vocab">Intercept: </font> When $\log(x)=0$, $(x=1)$, $y$ is expected to be $\beta_0$ (i.e. the mean of $y$ is $\beta_0$)

--

- <font class="vocab">Slope: </font> When $x$ is multiplied by a factor of $\mathbf{C}$, $y$ is expected to change by $\boldsymbol{\beta_1}\mathbf{\log(C)}$ units, i.e. the mean of $y$ changes by $\boldsymbol{\beta_1}\mathbf{\log(C)}$
    - *Example*: when $x$ is multiplied by a factor of 2, $y$ is expected to change by $\boldsymbol{\beta_1}\mathbf{\log(2)}$ units

---

### Rate vs. log(Age)

```{r,echo=F}
ex0824 <- ex0824 %>% mutate(log.age = log(Age))
ggplot(data=ex0824,aes(x=log.age,y=Rate)) + 
  geom_point()  +
  ggtitle("Respiratory Rate vs. log(Age)") + 
  xlab("log(Age)")+
  ylab("Respiratory Rate")
```

---

### Rate vs. Age

```{r,echo=F}
mod3 <- lm(Rate ~ log.age,data=ex0824)
kable(tidy(mod3, conf.int=TRUE),format="html")
```
<br> 

.question[

1. Write the equation for the model of $y$ regressed on $\log(x)$. 

2. Interpret the intercept in the context of the problem. 

3. Interpret the slope in terms of how the mean respiratory rate changes when a child's age doubles. 

4. Suppose a doctor has a patient who is currently 3 years old. Will this model provide a reliable prediction of the child's respiratory rate when her age doubles? Why or why not?
]


---

class: middle 

See [Log Transformations in Linear Regression](https://github.com/sta210-fa19/supplemental-notes/blob/master/log-transformations.pdf) for more details about interpreting regression models with log-transformed variables.


---

class: middle, center

### Model Assessment & Selection
---

## Restaurant tips

What affects the amount customers tip at a restaurant?

- **Response:**
    - <font class="vocab">`Tip`</font>: amount of the tip
    
- **Predictors:**
    - <font class="vocab">`Party`</font>: number of people in the party
    - <font class="vocab">`Meal`</font>:  time of day (Lunch, Dinner, Late Night) 
    - <font class="vocab">`Age`</font>: age category of person paying the bill (Yadult, Middle, SenCit)

```{r echo = F}
tips <- read_csv("data/tip-data.csv") %>%
  filter(!is.na(Party))
```

---

### Examining the Response Variable

```{r echo = F}
ggplot(data = tips, aes(x = Tip)) +
  geom_histogram() +
  labs("Distribution of Tips")
```

---

### Response vs. Predictors

```{r echo = F}
p1 <- ggplot(data = tips, aes(x = Party, y = Tip)) +
  geom_point() +
  labs(title = "Tips vs. Number of People in the Party")

p2 <- ggplot(data = tips, aes(x = Meal, y = Tip)) +
  geom_boxplot() +
  labs(title = "Tips vs. Time of the Meal", 
       x = "Time of Day")

p3 <- ggplot(data = tips, aes(x = Age, y = Tip)) +
  geom_boxplot() +
  labs(title = "Tips vs. Age of Person who Pays", 
       x = "Age")

plot_grid(p1, p2, p3, ncol = 2)
```

---

### Restaurant tips: model

```{r}
model1 <- lm(Tip ~ Party + Meal + Age , data = tips)
kable(tidy(model1),format="html",digits=3)
```
<br><br>

<center>
**Is this the best model to explain variation in Tips?**
</font>


---

## ANOVA table for regression

We can use the Analysis of Variance (ANOVA) table to decompose the variability in our response variable


|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | $$\sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$ | $$p$$ | $$\frac{MSS}{p}$$ | $$\frac{MMS}{RMS}$$ | $$P(F > \text{F-Stat})$$ |
| Residual | $$\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2$$ | $$n-p-1$$ | $$\frac{RSS}{n-p-1}$$ |  |  |
| Total | $$\sum\limits_{i=1}^{n}(y_i - \bar{y})^2$$ | $$n-1$$ | $$\frac{TSS}{n-1}$$ |  |  |


The estimate of the regression variance, $\hat{\sigma}^2 = RMS$

---

## $R^2$ 

- **Recall**: $R^2$ is the proportion of the variation in the response variable explained by the regression model
<br>

--

- $R^2$ will always increase as we add more variables to the model 
  + If we add enough variables, we can always achieve $R^2=100\%$
<br>

--

- If we only use $R^2$ to choose a best fit model, we will be prone to choose the model with the most predictor variables

---


## Adjusted $R^2$

- <font class="vocab">Adjusted $R^2$</font>: a version of $R^2$ that penalizes for unnecessary predictor variables
<br> 

- Similar to $R^2$, it measures the proportion of variation in the response that is explained by the regression model 
<br>

- Differs from $R^2$ by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables

---

## $R^2$ and Adjusted $R^2$

$$R^2 = \frac{\text{Total Sum of Squares} - \text{Residual Sum of Squares}}{\text{Total Sum of Squares}}$$
<br>

--

.alert[
$$Adj. R^2 = \frac{\text{Total Mean Square} - \text{Residual Mean Square}}{\text{Total Mean Square}}$$
]
<br>

--

- $Adj. R^2$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!

--

- Use $R^2$ when describing the relationship between the response and predictor variables


---

## Restaurant tips: ANOVA

- <font class="vocab">R output</font>

```{r}
kable(anova(model1), format = "markdown", digits = 3)
```

--

- <font class="vocab">ANOVA table</font>

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `r 1188.63588 + 88.46005 + 13.03236` | 5 | `r (1188.63588 + 88.46005 + 13.03236)/5` | `r ((1188.63588 + 88.46005 + 13.03236)/5)/(622.97932/163)` | 0 |
| Residual | 622.97932	 | 163 | `r 622.97932/163`  |  |  |
| Total | `r 1188.63588 + 88.46005 + 13.03236 + 622.97932	`  | 168 |  |  |  |

---

### Calculating $R^2$ and Adj $R^2$

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `r 1188.63588 + 88.46005 + 13.03236` | 5 | `r (1188.63588 + 88.46005 + 13.03236)/5` | `r ((1188.63588 + 88.46005 + 13.03236)/5)/(622.97932/163)` | 0 |
| Residual | 622.97932	 | 163 | `r 622.97932/163`  |  |  |
| Total | `r 1188.63588 + 88.46005 + 13.03236 + 622.97932	`  | 168 |  |  |  |


```{r}
#r-squared
mss <- 1290.12829
rss <- 622.97932	
tss <- mss + rss
(r_sq <- (tss - rss)/tss)
```

--

```{r}
#adj r-squared
rms <- 3.821959	
tms <- tss/(nrow(tips)-1)
(adj_r_sq <- (tms - rms)/tms)
```

---

### Restaurant tips: $R^2$ and Adj. $R^2$

```{r}
glance(model1)
```
<br>

- Close values of $R^2$ and Adjusted $R^2$ indicate that the variables in the model are significant in understanding variation in the response, i.e. that there aren't a lot of unnecessary variables in the model

---

## ANOVA F Test

- Using the ANOVA table, we can test whether any variable in the model is a significant predictor of the response. We conduct this test using the following hypotheses: 

.alert[
$$\begin{aligned}&H_0: \beta_{1} =  \beta_{2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

<br>

- The statistic for this test is the $F$ test statistic in the ANOVA table 

- We calculate the p-value using an $F$ distribution with $p$ and $(n-p-1)$ degrees of freedom

---

## ANOVA F Test in R

```{r}
model0 <- lm(Tip ~ 1, data = tips)
```

--

```{r}
model1 <- lm(Tip ~ Party + Meal + Age , data = tips)
```

--

```{r}
kable(anova(model0, model1), format="markdown", digits = 3)
```

**At least one coefficient is non-zero, i.e. at least one predictor in the model is significant**

---

### Testing subset of coefficients

- Sometimes we want to test whether a subset of coefficients are all equal to 0

- This is often the case when we want test 
    - whether a categorical variable with $k$ levels is a significant predictor of the response
    - whether the interaction between a categorical and quantitative variable is significant

- To do so, we will use the  <font class="vocab3">Nested F Test </font> 

---

## Nested F Test

- Suppose we have a full and reduced model: 

$$\begin{aligned}&\text{Full}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q + \beta_{q+1} x_{q+1} + \dots \beta_p x_p \\
&\text{Red}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q\end{aligned}$$
<br>

- We want to test whether any of the variables $x_{q+1}, x_{q+2}, \ldots, x_p$ are significant predictors. To do so, we will test the hypothesis: 

.alert[
$$\begin{aligned}&H_0: \beta_{q+1} =  \beta_{q+2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

---

## Nested F Test

- The test statistic for this test is 


$$F = \frac{(RSS_{reduced} - RSS_{full})\big/(p_{full} - p_{reduced})}{RSS_{full}\big/(n-p_{full}-1)}$$
<br> 

- Calculate the p-value using the F distribution with $(p_{full} - p_{reduced})$ and $(n-p_{full}-1)$ degrees of freedom

---

### Is `Meal` a significant predictor of tips?

```{r echo=F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```

---

### Tips data:  Nested F Test

$$\begin{aligned}&H_0: \beta_{late night} = \beta_{lunch} = 0\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$

--

```{r}
reduced <- lm(Tip ~ Party + Age, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
kable(anova(reduced, full), format="markdown", digits = 3)
```

--

**At least one coefficient associated with `Meal` is not zero. Therefore, `Meal` is a significant predictor of `Tips`.**

---

class: middle

.question[
Why is it not good practice to use the individual p-values to determine a categorical variable with $k > 2$ levels) is significant? 

*Hint*: What does it actually mean if none of the $k-1$ p-values are significant?
]

---

## Practice with Interactions 

```{r echo=F}
full <- lm(Tip ~ Party + Age + Meal + Meal*Party, data = tips)
kable(tidy(full),format="html")
```

.question[
1. What is the baseline level for `Meal`?
2. How do we expect the mean tips to change when `Meal == "Late Night"`, holding Age and Party constant? 
4. How does the slope of `Party` change when `Meal == "Late Night"`, holding Age and Party constant?
]
---

### Nested F test for interactions

**Are there any significant interaction effects with Party in the model?**

```{r}
reduced <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal + Age* Party + Meal * Party, 
           data = tips)
```

--

```{r}
kable(anova(reduced, full ), format="markdown", digits = 3)
```

---

### Final model for now

We conclude that there are no significant interactions with `Party` in the model. Therefore, we will use the original model that only included main effects. 


```{r echo  =F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```
