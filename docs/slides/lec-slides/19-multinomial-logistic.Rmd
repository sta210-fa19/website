---
title: "Multinomial Logistic Regression"
subtitle: "The Basics"
author: "Dr. Maria Tackett"
date: "11.04.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 4,
	fig.width = 7,
	message = FALSE,
	warning = FALSE
)
```

class: middle, center

### [Click for PDF of slides](19-multinomial-logistic.pdf)
---

### Announcements

- [Reading 11](https://www2.stat.duke.edu/courses/Fall19/sta210.001/reading/reading-11.html) for Monday

- Project Proposal **due TODAY at 11:59p**

- [Electronic Undergraduate Research Conference](https://www2.stat.duke.edu/courses/Fall19/sta199.001/slides/lec-slides/eusr_ad.pdf)

- HW 05 **due Wed, Nov 6 at 11:59p**



```{r, echo = F}
library(tidyverse)
library(knitr)
library(broom)
```

---

## Framingham Study

- Take about 5 minutes to wrap up your analysis on the Framingham Study

- Make sure at least one person has the final results saved in their RStudio Cloud project

---

class: middle, center

## Multinomial Logistic Regression

---

### Generalized Linear Models (GLM)

- In practice, there are many different types of response variables including:

  + **Binary**: Win or Lose
  + **Nominal**: Democrat, Republican or Third Party candidate
  + **Ordered**: Movie rating (1 - 5 stars)
  + and others...

- These are all examples of **generalized linear models**, a broader class of models that generalize the multiple linear regression model 

- See [*Generalized Linear Models: A Unifying Theory*](https://bookdown.org/roback/bookdown-bysh/ch-glms.html) for more details about GLMs





---

### Binary Response (Logistic)

- Given $P(Y_i=1|X_i)=p_i\hspace{5mm} \text{ and } \hspace{5mm}P(Y_i=0|X_i) = 1-p_i$

$$\log\Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 X_i$$
<br> 

- We can calculate $p_i$ by solving the logit equation: 

$$p_i = \frac{\exp\{\beta_0 + \beta_1 X_i\}}{1 + \exp\{\beta_0 + \beta_1 X_i\}}$$

---

 

### Binary Response (Logistic)

- Suppose we consider $Y=0$ the *<font color="blue">baseline category</font>* such that 

$$P(Y_i=0|X_i) = p_{i0} \text{ and } P(Y_i=1|X_i) = p_{i1}$$

--

- Then the logit model is 

$$\log\bigg(\frac{p_{i1}}{p_{i0}}\bigg) = \beta_0 + \beta_1 X_i$$

--

- <font class="vocab">Slope, $\beta_1$</font>: When $X$ increases by one unit, the odds of $Y=1$ versus the baseline $Y=0$ are expected to multiply by a factor of $\exp\{\beta_1\}$

- <font class="vocab3">Intercept, $\beta_0$</font>: When $X=0$, the odds of $Y=1$ versus the baseline $Y=0$ are expected to be $\exp\{\beta_0\}$

---

 

### Multinomial Logistic Regression 

- <font class="vocab">Multinomial Distribution: </font>

$$P(Y=1) = p_1, P(Y=2) = p_2, \ldots, P(Y=k) = p_k$$

such that $\sum\limits_{j=1}^{k} p_j = 1$
<br> 
<br> 

- Suppose we have no explanatory variables. What would be the best estimate of $p_j$?

---

 

### Multinomial Logistic Regression

- If we have an explanatory variable $X$, then we want $p_j$ to be a function of $X$

- Choose a baseline category. Let's choose $Y=1$.  Then, 

$$\log\bigg(\frac{p_{ij}}{p_{i1}}\bigg) = \beta_{0j} + \beta_{1j} X_i$$
<br>
--

- We have a separate model for each category of the response


---

### Multinomial Logistic Regression 

- Suppose we have a response variable $Y$ that can take three possible outcomes that are coded as 1,2,3

- Let 1 be the baseline category. Then 

$$\log\bigg(\frac{p_{i2}}{p_{i1}}\bigg) = \beta_{02} + \beta_{12} X_i \\[10pt]
\log\bigg(\frac{p_{i3}}{p_{i1}}\bigg) = \beta_{03} + \beta_{13} X_i$$

---

### Multinomial Regression in R 

- Use the <font class="vocab">`multinom()`</font> function in the `nnet` package 

```{r eval=F}
library(nnet)
my.model <- multinom(Y ~ X1 + X2 + ... + XP,data=my.data)
tidy(my.model, exponentiate = FALSE) #display log-odds model
```
<br> 

```{r eval=F}
# calculate predicted probabilities
pred.probs <- predict(my.model, type = "probs")
```

---

### Iris Data

- To illustrate multinomial logistic regression, we will use the <font class="vocab">`iris`</font> data set. 

- **Response:**
  + <font class="vocab">Species: </font>versicolor, setosa, virginica (baseline category)
  
- **Explanatory** (all measured in centimeters):
  + <font class="vocab">Sepal.Length</font>
  + <font class="vocab">Sepal.Width</font>
  + <font class="vocab">Petal.Length</font>
  + <font class="vocab">Petal.Width</font>

---

### Iris Data

```{r}
glimpse(iris)
```

---

### Iris Data

<center>
```{r, echo=FALSE,out.width = '70%'}
knitr::include_graphics("img/17/iris_parts.png")
```
</center>

---

### Iris Data

- <font class="vocab">Goal: </font> Use characteristics of the petals and sepal to distinguish between the three species

<center>
```{r, echo=FALSE,out.width = '80%'}
knitr::include_graphics("img/17/iris_type.jpg")
```
</center>

---

### Iris: Model 

**Suppose we have already done exploratory data analysis.**
<br>

```{r, results = "hide"}
library(nnet)
model1 <- multinom(Species ~ Sepal.Length + Sepal.Width + 
                     Petal.Length + Petal.Width, data = iris)
```

---

### Iris: Model 

```{r}
kable(tidy(model1, exponentiate = FALSE), format = "markdown", 
      digits = 3)
```

---

### Iris: Interpreting Coefficients

.question[
1. What is the baseline category? 

2. Write the model for the odds than an iris is a `versicolor` versus the baseline species.

3. Interpret the coefficient for `Sepal.Length` in terms of the odds that an iris is a `versicolor` versus the baseline species.
]

---

## Calculating Probabilities

- For $j=2,\ldots,k$, we calculate the probabilities, $p_{ij}$ as 

$$p_{ij} = \frac{\exp\{\beta_{0j} + \beta_{1j}X_i\}}{1 + \sum\limits_{j=2}^k \exp\{\beta_{0j} + \beta_{1j}X_i\}}$$
<br> 
<br>

- For the baseline category, $j=1$, we calculate the probability $p_{i1}$ as
$$p_{i1} = 1- \sum\limits_{j=2}^k p_{ij}$$
---

### Model Diagnostics

For each category of the response, $j$:

- Analyze a plot of the binned residuals vs. predicted probabilities 

- Analyze a plot of the binned residuals vs. each continuous predictor variable

- Look for any patterns in the residuals plots

- For each categorical predictor variable, examine the average residuals for each category of the response variable

---

### Drop-in-deviance Test

- Suppose there are two models: 
    - Model 1 includes predictors $x_1, \ldots, x_q$
    - Model 2 includes predictors $x_1, \ldots, x_q, x_{q+1}, \ldots, x_p$

- We want to test the hypotheses
$$\begin{aligned}&H_0: \beta_{q+1} = \dots = \beta_p = 0 \\
& H_a: \text{ at least 1 }\beta_j \text{ is not} 0\end{aligned}$$

- Use the **drop-in-deviance test** to compare models (similar to logistic regression)

---

### Iris: Predicted probabilities & residuals

```{r}
#calculate predicted probabilities
pred_probs <- data.frame(predict(model1, type = "probs"))
```

--

```{r}
iris <- iris %>%
  mutate(virginica = ifelse(Species == "virginica", 1, 0), 
         versicolor = ifelse(Species == "versicolor", 1, 0), 
         setosa = ifelse(Species == "setosa", 1, 0), 
         resid_virginica =  virginica - pred_probs$virginica, 
         resid_versicolor = versicolor - pred_probs$versicolor, 
         resid_setosa = setosa - pred_probs$setosa)
```

---

### Binned Residuals vs. Predictors

- Below are the plots for *Sepal.Length* These plots should be repeated for each explanatory variable

.small[
```{r echo = F}
#binned residuals vs. predictors - Virginica
par(mfrow=c(2,2))
arm::binnedplot(x = iris$Sepal.Length, y = iris$resid_virginica, 
                xlab="Sepal Length", 
                ylab="Residuals", 
                main="Virginica: Residuals vs. Sepal Length")

arm::binnedplot(x = iris$Sepal.Length, y = iris$resid_versicolor, 
                xlab="Sepal Length", 
                ylab="Residuals", 
                main="Versicolor: Residuals vs. Sepal Length")

arm::binnedplot(x = iris$Sepal.Length, y = iris$resid_setosa,
                xlab="Sepal Length", 
                ylab="Residuals", 
                main="Setosa: Residuals vs. Sepal Length")
```
]

---

### Interactions with Sepal.Length? 

We want to test if there are any significant interactions with `Sepal.Length`

.small[
```{r results="hide"}
#test for Sepal.Length interactions
model1 <- multinom(Species ~ Sepal.Length + Sepal.Width + 
                     Petal.Length + Petal.Width,data=iris)

model2 <- multinom(Species ~ Sepal.Length + Sepal.Width + Petal.Length +
                     Petal.Width + 
                     Sepal.Length * Sepal.Width + 
                     Sepal.Length * Petal.Length +
                     Sepal.Length * Petal.Width,
                   data=iris)
```
]

---

### Interactions with Sepal.Length? 

```{r}
anova(model1,model2,test="Chisq")
```
<br> 

--

**No significant interactions with Sepal.Length**

---

### Actual vs. Predicted Species

- We can use our model to predict the species

- The predicted species is the one with the highest predicted probability

```{r}
pred_species <- predict(model1, type = "class") 
table(iris$Species, pred_species)
```


